{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNh1Vqp4ccXWe0T//wtkXgb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ifeyichukwu/AfternoonSessionSEPLP/blob/master/SDG_17_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "collapsed": true,
        "id": "vK_422hzM-Q-",
        "outputId": "7af6bcb1-9c00-4977-b6e0-7dc7f84fc463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ SDG ML MODELING PIPELINE\n",
            "========================================\n",
            "\n",
            "1. Loading processed data...\n",
            "‚ùå Error loading data: [Errno 2] No such file or directory: 'data/sdg_processed_data.csv'\n",
            "\n",
            "2. Preparing ML features...\n",
            "‚ùå No data loaded. Please load data first.\n",
            "\n",
            "3. Training prediction models...\n",
            "‚ùå No features prepared. Please run prepare_features() first.\n",
            "\n",
            "4. Creating prediction visualizations...\n",
            "‚ùå No models trained. Please run train_prediction_models() first.\n",
            "\n",
            "5. Predicting future SDG scores...\n",
            "‚ùå No models trained. Please run train_prediction_models() first.\n",
            "\n",
            "6. Visualizing future predictions...\n",
            "‚ùå No future predictions available. Please run predict_future_scores() first.\n",
            "\n",
            "7. Generating collaboration recommendations...\n",
            "‚ùå No models trained. Please run train_prediction_models() first.\n",
            "\n",
            "ü§ù COLLABORATION RECOMMENDATIONS\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'groupby'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ad05bdab19cf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Run the complete ML pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mml_modeler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-ad05bdab19cf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;31m# Generate recommendations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n7. Generating collaboration recommendations...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m     \u001b[0mrecommendations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodeler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_collaboration_recommendations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n‚úÖ ML modeling complete! Check the images folder for visualizations.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-ad05bdab19cf>\u001b[0m in \u001b[0;36mgenerate_collaboration_recommendations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;31m# Get latest historical scores and future predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0mlatest_historical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Country'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SDG_Score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0mlatest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Country'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'groupby'"
          ]
        }
      ],
      "source": [
        "# SDG17 ML Collaboration - Advanced Modeling Notebook\n",
        "# This notebook contains advanced machine learning models for SDG analysis\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "class SDGMLModeler:\n",
        "    \"\"\"\n",
        "    Advanced ML modeling class for SDG collaboration analysis\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_path='data/sdg_processed_data.csv'):\n",
        "        \"\"\"Initialize with processed data\"\"\"\n",
        "        self.data_path = data_path\n",
        "        self.data = None\n",
        "        self.models = {}\n",
        "        self.predictions = {}\n",
        "        self.feature_importance = {}\n",
        "\n",
        "    def load_processed_data(self):\n",
        "        \"\"\"Load the processed SDG data\"\"\"\n",
        "        try:\n",
        "            self.data = pd.read_csv(self.data_path)\n",
        "            print(f\"‚úÖ Loaded processed data: {self.data.shape}\")\n",
        "            print(f\"Columns: {list(self.data.columns)}\")\n",
        "            return self.data\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def prepare_features(self):\n",
        "        \"\"\"Prepare features for machine learning models\"\"\"\n",
        "        if self.data is None:\n",
        "            print(\"‚ùå No data loaded. Please load data first.\")\n",
        "            return None\n",
        "\n",
        "        # Create feature matrix\n",
        "        feature_data = []\n",
        "\n",
        "        for country in self.data['Country'].unique():\n",
        "            country_data = self.data[self.data['Country'] == country].sort_values('Year')\n",
        "\n",
        "            if len(country_data) >= 5:  # Need at least 5 years of data\n",
        "                for i in range(4, len(country_data)):  # Start from 5th year\n",
        "                    # Historical features (last 4 years)\n",
        "                    hist_scores = country_data.iloc[i-4:i]['SDG_Score'].values\n",
        "                    hist_changes = country_data.iloc[i-3:i]['Score_Change'].values\n",
        "\n",
        "                    # Calculate trend features\n",
        "                    trend_slope = np.polyfit(range(4), hist_scores, 1)[0]\n",
        "                    trend_acceleration = np.polyfit(range(3), hist_changes, 1)[0]\n",
        "                    score_volatility = np.std(hist_scores)\n",
        "                    recent_momentum = np.mean(hist_changes[-2:])\n",
        "\n",
        "                    # Target (next year's score)\n",
        "                    target_score = country_data.iloc[i]['SDG_Score']\n",
        "                    current_year = country_data.iloc[i]['Year']\n",
        "\n",
        "                    feature_row = {\n",
        "                        'Country': country,\n",
        "                        'Year': current_year,\n",
        "                        'Current_Score': hist_scores[-1],\n",
        "                        'Score_1yr_ago': hist_scores[-2],\n",
        "                        'Score_2yr_ago': hist_scores[-3],\n",
        "                        'Score_3yr_ago': hist_scores[-4],\n",
        "                        'Recent_Change': hist_changes[-1],\n",
        "                        'Avg_Change_2yr': np.mean(hist_changes[-2:]),\n",
        "                        'Avg_Change_3yr': np.mean(hist_changes),\n",
        "                        'Trend_Slope': trend_slope,\n",
        "                        'Trend_Acceleration': trend_acceleration,\n",
        "                        'Score_Volatility': score_volatility,\n",
        "                        'Recent_Momentum': recent_momentum,\n",
        "                        'Target_Score': target_score\n",
        "                    }\n",
        "\n",
        "                    feature_data.append(feature_row)\n",
        "\n",
        "        self.feature_df = pd.DataFrame(feature_data)\n",
        "        print(f\"‚úÖ Features prepared: {self.feature_df.shape}\")\n",
        "        print(f\"Feature columns: {[col for col in self.feature_df.columns if col not in ['Country', 'Year', 'Target_Score']]}\")\n",
        "\n",
        "        return self.feature_df\n",
        "\n",
        "    def train_prediction_models(self):\n",
        "        \"\"\"Train multiple ML models to predict SDG scores\"\"\"\n",
        "        if not hasattr(self, 'feature_df') or self.feature_df is None:\n",
        "            print(\"‚ùå No features prepared. Please run prepare_features() first.\")\n",
        "            return None\n",
        "\n",
        "        # Prepare training data\n",
        "        feature_cols = [col for col in self.feature_df.columns\n",
        "                       if col not in ['Country', 'Year', 'Target_Score']]\n",
        "\n",
        "        X = self.feature_df[feature_cols]\n",
        "        y = self.feature_df['Target_Score']\n",
        "\n",
        "        # Split data (use recent years for testing)\n",
        "        test_mask = self.feature_df['Year'] >= 2018\n",
        "        X_train, X_test = X[~test_mask], X[test_mask]\n",
        "        y_train, y_test = y[~test_mask], y[test_mask]\n",
        "\n",
        "        print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "\n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Define models to train\n",
        "        models_to_train = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'Ridge Regression': Ridge(alpha=1.0),\n",
        "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "        }\n",
        "\n",
        "        # Train and evaluate models\n",
        "        results = {}\n",
        "\n",
        "        for name, model in models_to_train.items():\n",
        "            print(f\"\\nü§ñ Training {name}...\")\n",
        "\n",
        "            # Use scaled data for linear models, original for tree-based\n",
        "            if 'Regression' in name and 'Random' not in name and 'Gradient' not in name:\n",
        "                model.fit(X_train_scaled, y_train)\n",
        "                y_pred = model.predict(X_test_scaled)\n",
        "            else:\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "\n",
        "            # Calculate metrics\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            rmse = np.sqrt(mse)\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "            results[name] = {\n",
        "                'model': model,\n",
        "                'predictions': y_pred,\n",
        "                'mse': mse,\n",
        "                'rmse': rmse,\n",
        "                'mae': mae,\n",
        "                'r2': r2\n",
        "            }\n",
        "\n",
        "            print(f\"   RMSE: {rmse:.3f}\")\n",
        "            print(f\"   MAE: {mae:.3f}\")\n",
        "            print(f\"   R¬≤: {r2:.3f}\")\n",
        "\n",
        "            # Store feature importance for tree-based models\n",
        "            if hasattr(model, 'feature_importances_'):\n",
        "                importance_df = pd.DataFrame({\n",
        "                    'feature': feature_cols,\n",
        "                    'importance': model.feature_importances_\n",
        "                }).sort_values('importance', ascending=False)\n",
        "\n",
        "                self.feature_importance[name] = importance_df\n",
        "\n",
        "        self.models = results\n",
        "        self.scaler = scaler\n",
        "        return results\n",
        "\n",
        "    def create_prediction_visualizations(self):\n",
        "        \"\"\"Create visualizations for model predictions\"\"\"\n",
        "        if not self.models:\n",
        "            print(\"‚ùå No models trained. Please run train_prediction_models() first.\")\n",
        "            return None\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        # Get test data for comparison\n",
        "        test_mask = self.feature_df['Year'] >= 2018\n",
        "        y_true = self.feature_df[test_mask]['Target_Score']\n",
        "\n",
        "        for i, (name, results) in enumerate(self.models.items()):\n",
        "            y_pred = results['predictions']\n",
        "\n",
        "            # Prediction vs Actual scatter plot\n",
        "            axes[i].scatter(y_true, y_pred, alpha=0.6, s=50)\n",
        "            axes[i].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
        "            axes[i].set_xlabel('Actual SDG Score')\n",
        "            axes[i].set_ylabel('Predicted SDG Score')\n",
        "            axes[i].set_title(f'{name}\\nR¬≤ = {results[\"r2\"]:.3f}, RMSE = {results[\"rmse\"]:.3f}')\n",
        "            axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('images/prediction_performance.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        # Feature importance plot for tree-based models\n",
        "        if self.feature_importance:\n",
        "            fig, axes = plt.subplots(1, len(self.feature_importance), figsize=(15, 6))\n",
        "            if len(self.feature_importance) == 1:\n",
        "                axes = [axes]\n",
        "\n",
        "            for i, (model_name, importance_df) in enumerate(self.feature_importance.items()):\n",
        "                top_features = importance_df.head(8)\n",
        "\n",
        "                axes[i].barh(range(len(top_features)), top_features['importance'])\n",
        "                axes[i].set_yticks(range(len(top_features)))\n",
        "                axes[i].set_yticklabels(top_features['feature'])\n",
        "                axes[i].set_xlabel('Feature Importance')\n",
        "                axes[i].set_title(f'{model_name}\\nTop Features')\n",
        "                axes[i].invert_yaxis()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('images/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "    def predict_future_scores(self, years_ahead=3):\n",
        "        \"\"\"Predict SDG scores for future years\"\"\"\n",
        "        if not self.models:\n",
        "            print(\"‚ùå No models trained. Please run train_prediction_models() first.\")\n",
        "            return None\n",
        "\n",
        "        # Use the best performing model (highest R¬≤)\n",
        "        best_model_name = max(self.models.keys(), key=lambda x: self.models[x]['r2'])\n",
        "        best_model = self.models[best_model_name]['model']\n",
        "\n",
        "        print(f\"üîÆ Using {best_model_name} for future predictions (R¬≤ = {self.models[best_model_name]['r2']:.3f})\")\n",
        "\n",
        "        # Get latest data for each country\n",
        "        latest_data = self.data.groupby('Country').tail(5)  # Last 5 years for each country\n",
        "\n",
        "        future_predictions = []\n",
        "\n",
        "        for country in self.data['Country'].unique():\n",
        "            country_data = self.data[self.data['Country'] == country].sort_values('Year').tail(5)\n",
        "\n",
        "            if len(country_data) >= 4:\n",
        "                # Start with current data\n",
        "                current_scores = country_data['SDG_Score'].values\n",
        "                current_changes = country_data['Score_Change'].values[1:]  # Skip first NaN\n",
        "\n",
        "                for year_ahead in range(1, years_ahead + 1):\n",
        "                    # Prepare features for prediction\n",
        "                    hist_scores = current_scores[-4:]\n",
        "                    hist_changes = current_changes[-3:] if len(current_changes) >= 3 else current_changes\n",
        "\n",
        "                    trend_slope = np.polyfit(range(4), hist_scores, 1)[0]\n",
        "                    trend_acceleration = np.polyfit(range(len(hist_changes)), hist_changes, 1)[0] if len(hist_changes) > 1 else 0\n",
        "                    score_volatility = np.std(hist_scores)\n",
        "                    recent_momentum = np.mean(hist_changes[-2:]) if len(hist_changes) >= 2 else hist_changes[-1] if len(hist_changes) > 0 else 0\n",
        "\n",
        "                    feature_row = np.array([\n",
        "                        hist_scores[-1],  # Current_Score\n",
        "                        hist_scores[-2],  # Score_1yr_ago\n",
        "                        hist_scores[-3],  # Score_2yr_ago\n",
        "                        hist_scores[-4],  # Score_3yr_ago\n",
        "                        hist_changes[-1] if len(hist_changes) > 0 else 0,  # Recent_Change\n",
        "                        np.mean(hist_changes[-2:]) if len(hist_changes) >= 2 else hist_changes[-1] if len(hist_changes) > 0 else 0,  # Avg_Change_2yr\n",
        "                        np.mean(hist_changes) if len(hist_changes) > 0 else 0,  # Avg_Change_3yr\n",
        "                        trend_slope,\n",
        "                        trend_acceleration,\n",
        "                        score_volatility,\n",
        "                        recent_momentum\n",
        "                    ]).reshape(1, -1)\n",
        "\n",
        "                    # Make prediction\n",
        "                    if 'Regression' in best_model_name and 'Random' not in best_model_name and 'Gradient' not in best_model_name:\n",
        "                        predicted_score = best_model.predict(self.scaler.transform(feature_row))[0]\n",
        "                    else:\n",
        "                        predicted_score = best_model.predict(feature_row)[0]\n",
        "\n",
        "                    # Ensure reasonable bounds (SDG scores are typically 40-90)\n",
        "                    predicted_score = np.clip(predicted_score, 40, 95)\n",
        "\n",
        "                    future_predictions.append({\n",
        "                        'Country': country,\n",
        "                        'Year': 2022 + year_ahead - 1,\n",
        "                        'Predicted_Score': predicted_score,\n",
        "                        'Model_Used': best_model_name\n",
        "                    })\n",
        "\n",
        "                    # Update arrays for next iteration\n",
        "                    current_scores = np.append(current_scores[1:], predicted_score)\n",
        "                    if len(current_changes) > 0:\n",
        "                        new_change = predicted_score - hist_scores[-1]\n",
        "                        current_changes = np.append(current_changes[1:], new_change)\n",
        "\n",
        "        self.future_predictions = pd.DataFrame(future_predictions)\n",
        "        print(f\"‚úÖ Future predictions generated for {len(self.data['Country'].unique())} countries\")\n",
        "\n",
        "        return self.future_predictions\n",
        "\n",
        "    def visualize_future_predictions(self):\n",
        "        \"\"\"Visualize future predictions alongside historical data\"\"\"\n",
        "        if not hasattr(self, 'future_predictions') or self.future_predictions is None:\n",
        "            print(\"‚ùå No future predictions available. Please run predict_future_scores() first.\")\n",
        "            return None\n",
        "\n",
        "        # Select interesting countries to visualize\n",
        "        countries_to_plot = ['Germany', 'United States', 'China', 'India', 'Brazil', 'South Africa']\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for i, country in enumerate(countries_to_plot):\n",
        "            if country in self.data['Country'].values:\n",
        "                # Historical data\n",
        "                hist_data = self.data[self.data['Country'] == country]\n",
        "\n",
        "                # Future predictions\n",
        "                future_data = self.future_predictions[self.future_predictions['Country'] == country]\n",
        "\n",
        "                # Plot historical\n",
        "                axes[i].plot(hist_data['Year'], hist_data['SDG_Score'],\n",
        "                           marker='o', linewidth=2, label='Historical', color='blue')\n",
        "\n",
        "                # Plot predictions\n",
        "                if len(future_data) > 0:\n",
        "                    # Connect last historical point to first prediction\n",
        "                    last_hist_year = hist_data['Year'].max()\n",
        "                    last_hist_score = hist_data[hist_data['Year'] == last_hist_year]['SDG_Score'].iloc[0]\n",
        "\n",
        "                    pred_years = np.concatenate([[last_hist_year], future_data['Year'].values])\n",
        "                    pred_scores = np.concatenate([[last_hist_score], future_data['Predicted_Score'].values])\n",
        "\n",
        "                    axes[i].plot(pred_years, pred_scores,\n",
        "                               marker='s', linewidth=2, linestyle='--', label='Predicted', color='red')\n",
        "\n",
        "                axes[i].set_title(f'{country}', fontsize=14, fontweight='bold')\n",
        "                axes[i].set_xlabel('Year')\n",
        "                axes[i].set_ylabel('SDG Score')\n",
        "                axes[i].legend()\n",
        "                axes[i].grid(True, alpha=0.3)\n",
        "                axes[i].set_xlim(2015, 2025)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('images/future_predictions.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        # Summary table of predictions\n",
        "        print(\"\\nüìä FUTURE SDG SCORE PREDICTIONS\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        latest_predictions = self.future_predictions[self.future_predictions['Year'] == self.future_predictions['Year'].max()]\n",
        "        latest_predictions_sorted = latest_predictions.sort_values('Predicted_Score', ascending=False)\n",
        "\n",
        "        print(f\"Predictions for {latest_predictions['Year'].iloc[0]}:\")\n",
        "        for _, row in latest_predictions_sorted.iterrows():\n",
        "            print(f\"  {row['Country']}: {row['Predicted_Score']:.1f}\")\n",
        "\n",
        "    def generate_collaboration_recommendations(self):\n",
        "        \"\"\"Generate data-driven collaboration recommendations\"\"\"\n",
        "        if not hasattr(self, 'future_predictions'):\n",
        "            self.predict_future_scores()\n",
        "\n",
        "        print(\"\\nü§ù COLLABORATION RECOMMENDATIONS\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Get latest historical scores and future predictions\n",
        "        latest_historical = self.data.groupby('Country')['SDG_Score'].last()\n",
        "        latest_predictions = self.future_predictions.groupby('Country')['Predicted_Score'].last()\n",
        "\n",
        "        # Combine data\n",
        "        collab_data = pd.DataFrame({\n",
        "            'Current_Score': latest_historical,\n",
        "            'Predicted_Score': latest_predictions,\n",
        "            'Predicted_Change': latest_predictions - latest_historical\n",
        "        }).reset_index()\n",
        "\n",
        "        # Categorize countries\n",
        "        high_performers = collab_data[collab_data['Current_Score'] >= 78]['Country'].tolist()\n",
        "        emerging_strong = collab_data[(collab_data['Current_Score'] >= 70) & (collab_data['Current_Score'] < 78)]['Country'].tolist()\n",
        "        developing_focus = collab_data[collab_data['Current_Score'] < 70]['Country'].tolist()\n",
        "\n",
        "        declining_countries = collab_data[collab_data['Predicted_Change'] < -0.5]['Country'].tolist()\n",
        "        improving_countries = collab_data[collab_data['Predicted_Change'] > 1]['Country'].tolist()\n",
        "\n",
        "        print(\"1. üèÜ MENTORSHIP OPPORTUNITIES:\")\n",
        "        print(f\"   High Performers: {', '.join(high_performers)}\")\n",
        "        print(f\"   Can mentor: {', '.join(developing_focus)}\")\n",
        "\n",
        "        print(f\"\\n2. üöÄ PEER LEARNING NETWORKS:\")\n",
        "        print(f\"   Emerging Strong: {', '.join(emerging_strong)}\")\n",
        "        print(\"   ‚Üí Form regional collaboration networks\")\n",
        "\n",
        "        print(f\"\\n3. ‚ö†Ô∏è  PRIORITY INTERVENTION:\")\n",
        "        if declining_countries:\n",
        "            print(f\"   Countries needing support: {', '.join(declining_countries)}\")\n",
        "        else:\n",
        "            print(\"   No countries showing significant decline - good news!\")\n",
        "\n",
        "        print(f\"\\n4. üåü SUCCESS STORIES TO SHARE:\")\n",
        "        if improving_countries:\n",
        "            print(f\"   Fast improving countries: {', '.join(improving_countries)}\")\n",
        "            print(\"   ‚Üí Study their strategies for replication\")\n",
        "\n",
        "        return collab_data\n",
        "\n",
        "# Usage example\n",
        "def main():\n",
        "    \"\"\"Main function to demonstrate the ML modeling pipeline\"\"\"\n",
        "\n",
        "    print(\"ü§ñ SDG ML MODELING PIPELINE\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Initialize the modeler\n",
        "    modeler = SDGMLModeler()\n",
        "\n",
        "    # Load data\n",
        "    print(\"\\n1. Loading processed data...\")\n",
        "    modeler.load_processed_data()\n",
        "\n",
        "    # Prepare features\n",
        "    print(\"\\n2. Preparing ML features...\")\n",
        "    modeler.prepare_features()\n",
        "\n",
        "    # Train models\n",
        "    print(\"\\n3. Training prediction models...\")\n",
        "    modeler.train_prediction_models()\n",
        "\n",
        "    # Create visualizations\n",
        "    print(\"\\n4. Creating prediction visualizations...\")\n",
        "    modeler.create_prediction_visualizations()\n",
        "\n",
        "    # Predict future scores\n",
        "    print(\"\\n5. Predicting future SDG scores...\")\n",
        "    modeler.predict_future_scores(years_ahead=3)\n",
        "\n",
        "    # Visualize predictions\n",
        "    print(\"\\n6. Visualizing future predictions...\")\n",
        "    modeler.visualize_future_predictions()\n",
        "\n",
        "    # Generate recommendations\n",
        "    print(\"\\n7. Generating collaboration recommendations...\")\n",
        "    recommendations = modeler.generate_collaboration_recommendations()\n",
        "\n",
        "    print(f\"\\n‚úÖ ML modeling complete! Check the images folder for visualizations.\")\n",
        "\n",
        "    return modeler\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the complete ML pipeline\n",
        "    ml_modeler = main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m2ZLCM-8O8q7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}